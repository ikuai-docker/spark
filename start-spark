#!/usr/bin/env bash

# Set default HDFS user if not set
if [[ -z "${HDFS_USER}" ]]; then
  export HDFS_USER=spark
fi

/usr/sbin/sshd -D &

if [[ "${1}" = 'master' ]]; then

  # Start Hadoop NameNode
  start-hadoop namenode daemon
  # Start Spark Master
  spark-class org.apache.spark.deploy.master.Master -h $(hostname)
elif [[ "${1}" = 'worker' ]]; then

  # Repalce namenode host in core-site.xml
  sed -i.bak1 "s|\[NAMENODE_HOST\]|${2}|g" $HADOOP_CONF_DIR/core-site.xml
  rm -f $HADOOP_CONF_DIR/core-site.xml.bak

  # Repalce namenode host in yarn-site.xml
  sed -i.bak1 "s|\[NAMENODE_HOST\]|${2}|g" $HADOOP_CONF_DIR/yarn-site.xml
  rm -f $HADOOP_CONF_DIR/yarn-site.xml.bak

  # Wait for the Spark Master/Hadoop Namenode to start
  while ! nc -z $2 7077 || ! nc -z $2 50070 ; do
    sleep 2;
  done;
  # Start Hadoop DataNode
  start-hadoop datanode $2 daemon
  # Start Spark Worker
  spark-class org.apache.spark.deploy.worker.Worker spark://$2:7077
else
  echo "Invalid command '${1}'" >&2
  exit 1
fi
